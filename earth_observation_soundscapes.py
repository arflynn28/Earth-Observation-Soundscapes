# -*- coding: utf-8 -*-
"""Earth_Observation_Soundscapes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BbXLexLYTrugdmbe5bTVYLec0nZ8E1mF
"""

import os
import ee
import geemap
!pip install rasterio
import numpy as np
import rasterio

ee.Authenticate()
ee.Initialize(project="ee-arflynn28")  # <==== Replace this with your own EE project string

"""### Datasets"""

Map = geemap.Map()

# Elevation
dataset = ee.ImageCollection('JAXA/ALOS/AW3D30/V3_2');
elevation = dataset.select('DSM')
elevationVis = {
  'min': '0',
  'max': '5000',
  'palette': ['0000ff', '00ffff', 'ffff00', 'ff0000', 'ffffff']
}

# SAR
# fetch vv and vh data
def mask_edge(image):
  edge = image.lt(-30.0)
  masked_image = image.mask().And(edge.Not())
  return image.updateMask(masked_image)

img_vv = (
    ee.ImageCollection('COPERNICUS/S1_GRD')
    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))
    .filter(ee.Filter.eq('instrumentMode', 'IW'))
    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))
    .select('VV')
    .map(mask_edge)
)

# dates bounding the peak flood event
date_wet_start=ee.Date.fromYMD(2020,1,1)
date_wet_end=ee.Date.fromYMD(2020,12,30)

# filter the image collection for the dates during flood
# calculate the mean backscatter composite for this filtered collection
vv = img_vv.filterDate(date_wet_start,date_wet_end).mean()

# SAR despeckling
vv = vv.focalMean(1, 'circle', 'pixels')

# Cloud Frequency
# Define date range
start_date = '2022-01-01'
end_date = '2022-12-31'

# Load Sentinel-2 SR image collection
s2 = (ee.ImageCollection("COPERNICUS/S2_SR")
      .filterDate(start_date, end_date)
      .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 80)))

# Function to classify cloudy pixels using SCL band
def classify_clouds(img):
    scl = img.select('SCL')
    cloudy = (scl.eq(3)
              .Or(scl.eq(8))
              .Or(scl.eq(9))
              .Or(scl.eq(10))
              .Or(scl.eq(11)))
    return cloudy.rename('cloudy').toByte()

# Map cloud classification over image collection
cloudy_pixels = s2.map(classify_clouds)

# Total and cloudy image counts per pixel
total_images = cloudy_pixels.count()
cloud_sum = cloudy_pixels.sum()

# Cloud frequency calculation
cloud_frequency = cloud_sum.divide(total_images).multiply(100).rename('cloud_frequency')

# Land Surface Temperature
dataset = ee.ImageCollection('MODIS/061/MYD11A1').filter(ee.Filter.date('2018-01-01', '2018-05-01'));
landSurfaceTemperature = dataset.select('LST_Day_1km');
landSurfaceTemperatureVis = {
  'min': 13000.0,
  'max': 16500.0,
  'palette': [
    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',
    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',
    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',
    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',
    'ff0000', 'de0101', 'c21301', 'a71001', '911003'
  ],
};

# Night Lights
dataset = ee.ImageCollection('NOAA/VIIRS/DNB/ANNUAL_V22').filter(ee.Filter.date('2022-01-01', '2023-01-01'));
nighttime = dataset.select('average');
nighttimeVis = {min: 0.0, max: 60.0};

# Landcover
dataset = ee.ImageCollection('MODIS/061/MCD12Q1');
igbpLandCover = dataset.select('LC_Type1');
igbpLandCoverVis = {
  'min': 1.0,
  'max': 17.0,
  'palette': [
    '05450a', '086a10', '54a708', '78d203', '009900', 'c6b044', 'dcd159',
    'dade48', 'fbff13', 'b6ff05', '27ff87', 'c24f44', 'a5a5a5', 'ff6d4c',
    '69fff8', 'f9ffa4', '1c0dff'
  ],
};

# EVI
EVI = ee.ImageCollection('LANDSAT/COMPOSITES/C02/T1_L2_ANNUAL_EVI').filterDate('2023-01-01', '2023-12-31');
colorized = EVI.select('EVI');
colorizedVis = {
  'min': 0,
  'max': 1,
  'palette': [
    'ffffff', 'ce7e45', 'df923d', 'f1b555', 'fcd163', '99b718', '74a901',
    '66a000', '529400', '3e8601', '207401', '056201', '004c00', '023b01',
    '012e01', '011d01', '011301'
  ],
};

# Adding Map Layers
Map.addLayer(elevation, elevationVis, 'Elevation')
Map.addLayer(vv, {'min':-15,'max':-5}, 'VV')
Map.addLayer(cloud_frequency, {'min': 0,'max': 100,'palette': ['blue', 'white', 'gray']}, 'Cloud Frequency')
Map.addLayer(landSurfaceTemperature, landSurfaceTemperatureVis,'Land Surface Temperature')
Map.addLayer(nighttime, nighttimeVis, 'Nighttime')
Map.addLayer(igbpLandCover, igbpLandCoverVis, 'IGBP Land Cover')
Map.addLayer(colorized, colorizedVis, 'Colorized')

# Display the map
Map

"""### Defining Universal CRS"""

# Elevation (ALOS AW3D30)
elevation_crs = elevation.first().projection()
print("Elevation CRS:", elevation_crs.getInfo())

# VV_VH_WET (SAR data)
vv_crs = vv.projection()
print("VV CRS:", vv_crs.getInfo())

# Cloud Frequency
cloud_frequency_crs = cloud_frequency.projection()
print("Cloud Frequency CRS:", cloud_frequency_crs.getInfo())

# Land Surface Temperature
landSurfaceTemperature_crs = landSurfaceTemperature.first().projection()
print("Land Surface Temperature CRS:", landSurfaceTemperature_crs.getInfo())

# Nighttime Lights
nighttime_crs = nighttime.first().projection()
print("Nighttime Lights CRS:", nighttime_crs.getInfo())

# IGBP Land Cover
igbpLandCover_crs = igbpLandCover.first().projection()
print("IGBP Land Cover CRS:", igbpLandCover_crs.getInfo())

# Colorized - Use the first image from the collection to check CRS
colorized_crs = colorized.first().projection()
print("Colorized CRS:", colorized_crs.getInfo())

# Define the target CRS and scale (in meters)
target_crs = 'EPSG:4326'
target_scale = 50  # meters

# Reproject Land Surface Temperature
landSurfaceTemperature_reprojected = landSurfaceTemperature.mosaic().reproject(
    crs='EPSG:4326',
    scale=target_scale
)

# Reproject IGBP Land Cover
igbpLandCover_reprojected = igbpLandCover.map(lambda img: img.reproject(
    crs='EPSG:4326',
    scale=target_scale
))

# Add to Map for visualization
Map = geemap.Map()

Map.addLayer(landSurfaceTemperature_reprojected, landSurfaceTemperatureVis, 'LST (Reprojected)')
Map.addLayer(igbpLandCover_reprojected, igbpLandCoverVis, 'Land Cover (Reprojected)')

Map

"""### Normalizing the Datasets

#### Restricting Landcover Classification
"""

# Select the land cover classification band (e.g., 'LC_Type1')
landcover_collection = igbpLandCover_reprojected.select('LC_Type1')

# 1. First image (first image in the collection)
first_image = landcover_collection.first()

# Initialize map
Map = geemap.Map()

# Add layers to the map
Map.addLayer(first_image, igbpLandCoverVis, 'First Image (B4)')

# Display map
Map

"""#### Normalization"""

AOI = ee.Geometry.Polygon([[[-3.2911941601757344,51.75188489921971],
 [-3.0687210156444844,51.75188489921971],
  [-3.0687210156444844,51.87499002591345],
   [-3.2911941601757344,51.87499002591345]]])

# Define the normalization function
def normalize_ee_image(image, min_val=None, max_val=None, scale_min=0, scale_max=1):
    """
    Normalize a GEE image to a [scale_min, scale_max] range.

    Parameters:
    - image: GEE image object (ee.Image) or ImageCollection (ee.ImageCollection)
    - min_val: Minimum value for scaling. If None, it uses the image's min.
    - max_val: Maximum value for scaling. If None, it uses the image's max.
    - scale_min: Minimum value of the output range (default is 0).
    - scale_max: Maximum value of the output range (default is 1).

    Returns:
    - Normalized ee.Image or ee.ImageCollection
    """
    if isinstance(image, ee.ImageCollection):
        # If it's an ImageCollection, reduce it by median or another method (e.g., mean)
        image = image.median()  # Use median to reduce the collection to one image (you can use mean, etc.)

    # Define a global bounding box geometry (world extent)
    geometry = ee.Geometry.Polygon([[[-3.2911941601757344,51.75188489921971],
     [-3.0687210156444844,51.75188489921971],
      [-3.0687210156444844,51.87499002591345],
       [-3.2911941601757344,51.87499002591345]]])


    # If min and max values are not provided, calculate from the image itself
    if min_val is None:
        min_val = image.reduceRegion(
            reducer=ee.Reducer.min(),
            geometry=geometry,
            scale=50,  # Use an appropriate scale (modify based on your data's resolution)
            maxPixels=1e13  # For large datasets, increase maxPixels
        ).getInfo()

        print(f"Min value response: {min_val}")  # Debugging print statement

        if min_val is None or not min_val:
            raise ValueError("Unable to retrieve min value from the image.")

        min_val = min_val[list(min_val.keys())[0]]

    if max_val is None:
        max_val = image.reduceRegion(
            reducer=ee.Reducer.max(),
            geometry=geometry,
            scale=50,  # Use an appropriate scale (modify based on your data's resolution)
            maxPixels=1e13  # For large datasets, increase maxPixels
        ).getInfo()

        print(f"Max value response: {max_val}")  # Debugging print statement

        if max_val is None or not max_val:
            raise ValueError("Unable to retrieve max value from the image.")

        max_val = max_val[list(max_val.keys())[0]]

    # Avoid division by zero (if min and max are equal)
    if max_val - min_val == 0:
        raise ValueError("Image has no variation. Min and max values are the same.")

    # Normalize the image to [scale_min, scale_max]
    normalized_image = image.subtract(min_val).divide(max_val - min_val).multiply(scale_max - scale_min).add(scale_min)

    return normalized_image

Map = geemap.Map()
Map.centerObject(AOI, 10);

# Elevation
elevation_norm = normalize_ee_image(elevation, scale_min=0, scale_max=100)
elevation = elevation_norm.clip(AOI)
Map.addLayer(elevation, {'min': 0, 'max': 100}, 'Normalized Elevation')

# Land Surface Temperature
landSurfaceTemperature_norm = normalize_ee_image(landSurfaceTemperature_reprojected, scale_min=0, scale_max=100)
landSurfaceTemperature = landSurfaceTemperature_norm.clip(AOI)
Map.addLayer(landSurfaceTemperature, {'min': 0, 'max': 100, 'palette': ['blue', 'red']}, 'Normalized Land Surface Temperature')

# VV (SAR)
vv_norm = normalize_ee_image(vv, scale_min=0, scale_max=1)
SAR = vv_norm.clip(AOI)
Map.addLayer(SAR, {'min':0,'max':1, 'palette': ['black', 'white']}, 'Normalized VV')

# Cloud Frequency
cloud_frequency_norm = normalize_ee_image(cloud_frequency, scale_min=0, scale_max=1)
cloudfrequency = cloud_frequency_norm.clip(AOI)
Map.addLayer(cloudfrequency, {'min': 0, 'max': 1, 'palette': ['blue', 'white','gray']}, 'Normalized Cloud Frequency')

# Nighttime Lights
nighttime_image = nighttime.mosaic()
nighttime_norm = normalize_ee_image(nighttime_image, scale_min=0, scale_max=1)
nighttime_cut = nighttime_norm.clip(AOI)
Map.addLayer(nighttime_cut, nighttimeVis, 'Normalized Nighttime')

# IGBP LandCover
LandCover = first_image.clip(AOI)
Map.addLayer(LandCover, igbpLandCoverVis, 'Land Cover Classes')

# Colorized EVI
EVI_image = colorized.first()
EVI_norm = normalize_ee_image(EVI_image, scale_min=0, scale_max=1)
EVI = EVI_norm.clip(AOI)
Map.addLayer(EVI, colorizedVis, 'Normalized EVI')
Map

"""#### Extracting Pixel Values"""

# Assuming normalized datasets (elevation_norm, NDVI_norm, etc.) and igbpLandCover_reprojected are available from previous code
# Function to extract pixel values within the geometry
def extract_pixel_values(image, geometry, scale=50):
    """
    Extracts pixel values from an image within a specified geometry.

    Args:
        image: The Earth Engine image.
        geometry: The Earth Engine geometry.
        scale: The scale in meters for the reduction region.

    Returns:
        A NumPy array of pixel values.
    """

    # Reduce the image to get pixel values within the geometry
    pixel_values = image.reduceRegion(
        reducer=ee.Reducer.toList(),
        geometry=AOI,
        scale=scale,
        maxPixels=1e13
    ).getInfo()

    # Extract pixel values as a list
    if pixel_values:
      pixel_values_list = pixel_values[list(pixel_values.keys())[0]]
      return np.array(pixel_values_list)  # Convert to a NumPy array
    else:
      print(f"No data found within the specified geometry for image: {image}")
      return np.array([])  # return an empty array to avoid errors

# Extract pixel values for each dataset
elevation_pixels = extract_pixel_values(elevation, AOI)
temperature_pixels = extract_pixel_values(landSurfaceTemperature, AOI)
vv_pixels = extract_pixel_values(vv, AOI)
cloud_frequency_pixels = extract_pixel_values(cloudfrequency, AOI)
nighttime_pixels = extract_pixel_values(nighttime_cut, AOI)
EVI_pixels = extract_pixel_values(EVI, AOI)
landcover_pixels = extract_pixel_values(LandCover, AOI)

# Print or further process the extracted pixel values
print("Elevation Pixels:", elevation_pixels)
print("Land Surface Temperature Pixels:", temperature_pixels)
print("VV Pixels:", vv_pixels)
print("Cloud Frequency Pixels:", cloud_frequency_pixels)
print("Nighttime Pixels:", nighttime_pixels)
print("Colorized Pixels:", EVI_pixels)
print("Landcover Pixels:", landcover_pixels)

"""### Mapping to musical parameters"""

def set_pixel_shp(image, geometry, scale=50, width=None, preserve_classes=True):
    """
    Extracts pixel values from an image, reshapes them, and optionally normalizes the values to [0, 1].

    Args:
        image: The Earth Engine image.
        geometry: The geometry to extract pixel values from.
        scale: The scale in meters for the reduction region.
        width: The width (number of columns).
        preserve_classes: Whether to preserve the original class values or normalize to [0, 1].

    Returns:
        A NumPy array of pixel values (either original or normalized).
    """
    # Extract pixel values (same as before)
    pixel_values = image.reduceRegion(
        reducer=ee.Reducer.toList(),
        geometry=geometry,
        scale=scale,
        maxPixels=1e13
    ).getInfo()

    if pixel_values:
        pixel_values_list = pixel_values[list(pixel_values.keys())[0]]
        pixel_values_array = np.array(pixel_values_list)

        # Calculate the correct height based on the array size and width
        height = pixel_values_array.size // width  # Use integer division (//)

        # Trim or pad the array to make it divisible by width for proper reshaping
        remainder = pixel_values_array.size % width
        if remainder != 0:
            # Trim extra elements
            pixel_values_array = pixel_values_array[:-remainder]

        # Reshape the data to a 2D array (height x width)
        pixel_values_2d = pixel_values_array.reshape(height, width)

        # Optionally normalize the pixel values (if desired)
        if preserve_classes:
            # Do not normalize values if you want to keep them in their original classes
            return pixel_values_2d
        else:
            # Normalize the pixel values to [0, 1] for continuous data
            min_val = np.min(pixel_values_2d)
            max_val = np.max(pixel_values_2d)
            normalized_data = (pixel_values_2d - min_val) / (max_val - min_val)
            return normalized_data
    else:
        print(f"No data found within the specified geometry for image: {image}")
        return np.array([])  # Return an empty array if no data is found

# Define your geometry (AOI) and image (e.g., elevation or temperature)
normalized_elevation_pixels = set_pixel_shp(elevation, AOI, scale=50, width=495)
normalized_temperature_pixels = set_pixel_shp(landSurfaceTemperature, AOI, scale=50, width=495)
normalized_vv_pixels = set_pixel_shp(vv, AOI, scale=50, width=495)
normalized_cloud_frequency_pixels = set_pixel_shp(cloudfrequency, AOI, scale=50, width=495)
normalized_nighttime_pixels = set_pixel_shp(nighttime_cut, AOI, scale=50, width=495)
normalized_EVI_pixels = set_pixel_shp(EVI, AOI, scale=50, width=495)
normalized_landcover_pixels = set_pixel_shp(LandCover, AOI, scale=50, width=495)

# Print the normalized pixel arrays
print("Normalized Elevation Pixels:", normalized_elevation_pixels)
print("Normalized Elevation Pixels Shape:", normalized_elevation_pixels.shape)

print("Normalized Temperature Pixels:", normalized_temperature_pixels)
print("Normalized Temperature Pixels Shape:", normalized_temperature_pixels.shape)

print("Normalized VV Pixels:", normalized_vv_pixels)
print("Normalized VV Pixels Shape:", normalized_vv_pixels.shape)

print("Normalized Cloud Frequency Pixels:", normalized_cloud_frequency_pixels)
print("Normalized Cloud Frequency Pixels Shape:", normalized_cloud_frequency_pixels.shape)

print("Normalized Nighttime Pixels:", normalized_nighttime_pixels)
print("Normalized Nighttime Pixels Shape:", normalized_nighttime_pixels.shape)

print("Normalized EVI Pixels:", normalized_EVI_pixels)
print("Normalized EVI Pixels Shape:", normalized_EVI_pixels.shape)

print("Normalized Landcover Pixels:", normalized_landcover_pixels)
print("Normalized Landcover Pixels Shape:", normalized_landcover_pixels.shape)

"""#### Pitch"""

# Simulated raster arrays (normalized to 0–100, shape: [height, width])
# Replace these with your real `elevation_pixels` and `temperature_pixels` reshaped to (H, W)
elevation_pixels_2d = normalized_elevation_pixels.reshape(274, 495)
temperature_pixels_2d = normalized_temperature_pixels.reshape(274, 495)

# Define pitch range
min_pitch = 30
max_pitch = 90

# Weights for blending elevation and temperature
elevation_weight = 0.5
temperature_weight = 0.5

# Invert elevation so high elevation = low pitch
elevation_inverted_2d = 100 - elevation_pixels_2d

# Normalize both to 0–1
elevation_scaled_2d = elevation_inverted_2d / 100
temperature_scaled_2d = temperature_pixels_2d / 100

# Empty array to hold pitch per column
pitch_per_column = []

# Loop through each column (left to right)
for col in range(elevation_scaled_2d.shape[1]):
    # Get the column for both datasets
    elevation_col = elevation_scaled_2d[:, col]
    temperature_col = temperature_scaled_2d[:, col]

    # Use mean or median of the column as representative
    elevation_val = np.mean(elevation_col)
    temperature_val = np.mean(temperature_col)

    # Combine values using weights
    combined_val = (elevation_val * elevation_weight) + (temperature_val * temperature_weight)

    # Scale to pitch range
    pitch = combined_val * (max_pitch - min_pitch) + min_pitch
    pitch_per_column.append(pitch)

# Convert to array (or list of MIDI notes, frequencies, etc.)
pitch_per_column = np.array(pitch_per_column)

# Preview output
print("Pitch values (left to right):", pitch_per_column)

"""#### Rhythm"""

min_cloud = 0  # Min of cloud frequency
max_cloud = 1  # Max of cloud frequency
min_rhythm = 0  # Rhythm base (could represent faster tempo)
max_rhythm = 1  # Rhythm base (slower tempo or denser rhythm)

# Cloud Frequency to Rhythm Variation
cloud_scaled = (normalized_cloud_frequency_pixels - min_cloud) / (max_cloud - min_cloud)
rhythm_variation = (cloud_scaled * (max_rhythm - min_rhythm)) + min_rhythm

# Simulated raster arrays (normalized to 0–100, shape: [height, width])
cloud_pixels_2d = normalized_cloud_frequency_pixels.reshape(274, 495)

# Normalize both to 0–1
cloud_scaled_2d = normalized_cloud_frequency_pixels / 100

# Empty array to hold pitch per column
rhythm_per_column = []

# Loop through each column (left to right)
for col in range(cloud_scaled_2d.shape[1]):
    # Get the column for both datasets
    cloud_col = cloud_scaled_2d[:, col]

    # Use mean or median of the column as representative
    cloud_val = np.mean(cloud_col)

    # Scale to pitch range
    rhythm = cloud_val * (max_rhythm - min_rhythm) + min_rhythm
    rhythm_per_column.append(rhythm)

# Convert to array (or list of MIDI notes, frequencies, etc.)
rhythm_per_column = np.array(rhythm_per_column)

# Preview output
print("Pitch values (left to right):", rhythm_per_column)

"""#### Volume"""

# Assuming min and max values for volume scaling
min_vol = 0  # Minimum volume (quiet)
max_vol = 127  # Maximum volume (loud)

# Example of the EVI and nighttime scaling
landcover_pixels = normalized_landcover_pixels.astype(int)normalized_landcover_pixels.reshape(274, 495)
landcover_pixels_2d =
night_scaled_2d = normalized_nighttime_pixels / 100  # Normalize nighttime data (assuming already normalized)
EVI_scaled_2d = normalized_EVI_pixels / 100  # Normalize EVI data (assuming already normalized)

# Define class mapping for land cover
landcover_map = {
    1: "Forest",  # Forest class
    2: "Forest",  # Forest class
    3: "Forest",  # Forest class
    4: "Forest",  # Forest class
    5: "Forest",  # Forest class
    6: "Forest",  # Forest class
    7: "Forest",  # Forest class
    8: "Forest",  # Forest class
    9: "Forest",  # Forest class
    10: "Forest",  # Forest class
    11: "Forest",  # Forest class
    12: "Crops",   # Crops class
    13: "Urban",   # Urban class
    14: "Other",   # Other land cover
    15: "Other",   # Other land cover
    16: "Other",   # Other land cover
    17: "Special"  # Constant volume class
}

# Empty array to hold adjusted volumes per column
vol_per_column = []

# Loop through each column in the land cover data (left to right)
for col in range(normalized_landcover_pixels.shape[1]):
    column_volumes = []

    # Iterate over each row in the column (i.e., per pixel)
    for row in range(normalized_landcover_pixels.shape[0]):
        landcover_class = landcover_pixels_2d[row, col]  # Get the land cover class for the pixel

        # Get the relevant values for the column (EVI and nighttime)
        EVI_val = np.mean(EVI_scaled_2d[:, col])  # Example: EVI value for the column
        night_val = np.mean(night_scaled_2d[:, col])  # Example: nighttime value for the column

        # Set volume based on the land cover class
        if landcover_class == 17:  # Special class with constant volume
            volume_adjustment = 0.4  # Constant volume for class 17 (e.g., steady volume)
        elif landcover_class in list(range(12, 14)):  # Crops and Urban: use nighttime for volume
        # Increase volume based on nighttime value (urban/crops get higher volumes at night)
            combined_val = night_val * (max_vol - min_vol) + min_vol
            volume_adjustment = combined_val  # Apply the nighttime volume adjustment
        elif landcover_class in list(range(1, 12)) + list(range(14, 17)):  # Forest, Grassland, Other
        # Increase volume based on EVI (forests/vegetation areas)
            combined_val = EVI_val * (max_vol - min_vol) + min_vol
            volume_adjustment = combined_val  # Apply the EVI volume adjustment
        else:
            volume_adjustment = min_vol  # Default volume if no other condition is met (optional)

        # Append the volume adjustment for the current pixel
        column_volumes.append(volume_adjustment)

    # Store the adjusted volumes for each column
    vol_per_column.append(column_volumes)

# Convert to array for further processing
vol_per_column = np.array(vol_per_column)

# Preview the output
print("Adjusted Volumes (left to right):", vol_per_column)

"""#### Instrument Selection"""

height, width = normalized_landcover_pixels.shape
print(f"Height: {height}, Width: {width}")

unique_classes = np.unique(landcover_pixels)
print("Unique classes in landcover_pixels:", unique_classes)

# Simulated raster arrays (normalized to 0–100, shape: [height, width])
landcover_pixels = normalized_landcover_pixels.astype(int)
landcover_pixels_2d = normalized_landcover_pixels.reshape(274, 495)

# Threshold Pixel Column Requirements
threshold = 2

# Defining Classes Per Value
landcover_map = {
    1: "Cello",
    2: "viola",
    3: "Contrabassoon",
    4: "Oboe",
    5: "Bassoon",
    6: "French Horn",
    7: "Clarinet",
    8: "Flute",
    9: "Violin",
    10: "Saxophone",
    11: "Harp",
    12: "piano",
    13: "Synthesizer",
    14: "Trumpet",
    15: "Tuba",
    16: "Trombone",
    17: "String Bass",
}

column_instrument_data = []

for col in range (495):
  col_classes =landcover_pixels_2d[:, col]

  # Count number of pixels per class
  unique, counts = np.unique(col_classes, return_counts=True)
  class_counts = dict(zip(unique, counts))

  instruments_in_column = {}

  for class_code, count in class_counts.items():
    if count >= threshold:
      instrument = landcover_map.get(class_code, "Unknown")
      instruments_in_column[instrument] = {
          "count": count,
          "complexity": count / height
      }

  column_instrument_data.append(instruments_in_column)

# Convert to array (or list of MIDI notes, frequencies, etc.)
instrument_per_column = np.array(column_instrument_data)

# Preview output
print("Pitch values (left to right):", instrument_per_column)

"""#### Percussion"""

SAR_pixels_2d = normalized_vv_pixels.reshape(274, 495)

# Adjust the normalization for negative SAR values
min_SAR = np.min(SAR_pixels_2d)  # Find the minimum value in the SAR data
max_SAR = np.max(SAR_pixels_2d)  # Find the maximum value in the SAR data
print(max_SAR)
print(min_SAR)

# Shift the values to make them positive
shifted_SAR = SAR_pixels_2d - min_SAR  # Shift all values to be >= 0
normalized_SAR = shifted_SAR / (max_SAR - min_SAR)  # Normalize to the range [0, 1]

# Define percussion instrument map with a variety of percussion sounds
percussion_map = {
    "Snare Drum": 1,
    "Bass Drum": 2,
    "Cymbals": 3,
    "Hi-hat": 4,
    "Tom Drum": 5,
    "Bongo": 6,
    "Claps": 7,
    "Cowbell": 8
}

# Define SAR value thresholds for different percussion instruments
thresholds = {
    "Snare Drum": (0.0, 0.),  # Always added if SAR is above 0.0
    "Bass Drum": (0.21, 0.4),   # Added if SAR is between 0.3 and 0.6
    "Cymbals": (0.41, 0.6),     # Added if SAR is between 0.6 and 0.9
    "Hi-hat": (0.61, 0.7),
    "Tom Drum": (0.71, 0.8),
    "Bongo": (0.81, 0.9),
    "Claps": (0.91, 0.1),
    "Cowbell": (1.0, 1.0)
}

# Loop through each column (left to right) in normalized SAR data
percussion_instruments = []

for col in range(normalized_SAR.shape[1]):
    # Get the column's SAR values
    SAR_col = normalized_SAR[:, col]

    # Use the mean of the column for decision making
    SAR_val = np.mean(SAR_col)

    # Start with Snare Drum always being added
    assigned_instruments = ["Snare Drum"]

    # Add other instruments based on SAR value and thresholds
    for instrument, (min_range, max_range) in thresholds.items():
        if min_range <= SAR_val < max_range:
            assigned_instruments.append(instrument)

    percussion_instruments.append(assigned_instruments)

# Preview the output
print("Assigned Percussion Instruments per Column (left to right):", percussion_instruments)

"""### Mapping to Midi/Makin Music"""

